# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14gDii1Dke7fd39blfTnrPjCikH2oD-Z2

Razi Haider

19I-1762

DS-N

Data mining project

Libraries:
"""

import pandas as pd
import numpy as np
import re
import spacy
import torch
from PIL import Image
import torchvision.transforms as transforms
from sklearn.preprocessing import LabelEncoder
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import time

# from google.colab import drive
# drive.mount('/content/drive')

from skimage import data
from skimage import filters
from skimage.color import rgb2gray
import matplotlib.pyplot as plt

import os
from skimage import io
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from yellowbrick.cluster import KElbowVisualizer
import tensorflow as tf
from sklearn.metrics import accuracy_score

from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import img_to_array
from sklearn.cluster import KMeans
from tqdm import tqdm
import os
import shutil

import cv2

"""Enabling GPU:"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

# import matplotlib.cbook as cbook
# import matplotlib.image as image
# import matplotlib.pyplot as plt

"""Data loading"""

df = pd.read_csv('/content/drive/MyDrive/SEMESTER6/DM/Data_labels.csv')

df['Image_name'] = df['Image_name'] + '.jpg'

df

"""Flattening and resizing images for kmeans clustering"""

klist = []
for i in df['Image_name']:
  image = cv2.imread('/content/drive/MyDrive/SEMESTER6/DM/Images/' + i)
  image = cv2.resize(image, (100, 100), interpolation = cv2.INTER_AREA)
  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  klist.append(image.flatten())

"""I have commented this box because of its high run time. You can run this, but I have provided the graph itself for Kelbow."""

# m = KMeans()
# v = KElbowVisualizer(m, k=(2, 20))
# v.fit(np.array(klist))
# v.show()

"""According to the kelbow graph, k=7 is the best number for clustering"""

k = 7
clusters = KMeans(k, random_state = 40)
clusters.fit(klist)

"""Here I am creating a new dataframe which consists of clustered labels with k = 7."""

img_name = []
for i in df['Image_name']:
  img_name.append(i)

image_cluster = pd.DataFrame(img_name,columns=['image'])
image_cluster["clusterid"] = clusters.labels_
image_cluster

image_cluster.to_csv('clustered.csv', index=False)

"""The code below was used to segment images using kmeans with k=7 and to save the new segmented images to another folder (PIL_SEG)."""

# segArr = []
# for i in df['Image_name']:
#   print(i)
#   # Read in the image
#   image = cv2.imread('/content/drive/MyDrive/SEMESTER6/DM/Images/' + i)
#   image = cv2.resize(image, (100, 100), interpolation = cv2.INTER_AREA)
#   # Change color to RGB (from BGR)
#   image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
#   #plt.imshow(image)
#   pixel_vals = image.reshape((-1,3))
#   # Convert to float type
#   pixel_vals = np.float32(pixel_vals)
#   criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)
  
#   # then perform k-means clustering wit h number of clusters defined as 3
#   #also random centres are initially choosed for k-means clustering
#   k = 7
#   retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
  
#   # convert data into 8-bit values
#   centers = np.uint8(centers)
#   segmented_data = centers[labels.flatten()]
  
#   # reshape data into the original image dimensions
#   segmented_image = segmented_data.reshape((image.shape))
#   pil_im = Image.fromarray(segmented_image)
# # im_name = im +".jpg"
#   pil_im.save('/content/drive/MyDrive/SEMESTER6/DM/PIL_SEG/'+i)
#   # plt.imshow(segmented_image)
#   # plt.grid(False)
#   # plt.xticks([])
#   # plt.yticks([])
#   # plt.savefig('/content/drive/MyDrive/SEMESTER6/DM/Segments/'+i)

"""Select 70% data

I used a for loop to select 70% of rows of each label. I stored the newly generated data in another dataframe (new_df).
"""

new_df = pd.DataFrame(columns=['image', 'clusterid'])
for i in image_cluster['clusterid'].unique():
  temp = image_cluster[image_cluster['clusterid'] == i]
  per = int(len(temp)*0.7)
  tempdf = temp.sample(n = per)
  new_df = pd.concat([new_df, tempdf])

new_df

new_df = new_df.sort_index(ascending=True)
new_df

"""I stored the final dataframe as a csv in my folder so that I don't have to run previous code cells again. You can uncommented the last line of the following cell to save csv."""

image_cluster = new_df.reset_index(drop=True)
image_cluster['clusterid'] = image_cluster['clusterid'].astype('int')
#image_cluster.to_csv('/content/drive/MyDrive/SEMESTER6/DM/dataUsed1.csv', index=False)

# image_cluster

"""Loading new data"""

image_cluster = pd.read_csv('/content/drive/MyDrive/SEMESTER6/DM/dataUsed1.csv')

image_cluster

"""Non segment images

ANN

Custom dataloader which converts labels and images in the form suitable for neural networks.
"""

#reference: https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/custom_dataset_txt/loader_customtext.py

class ImageDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform = None, freq_threshold = 5):
        self.df = csv_file.copy()
        self.img = self.df['image']
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, index):
        image = Image.open(os.path.join(self.root_dir, self.df.iloc[index, 0])).convert("RGB")
        y_label = torch.tensor(int(self.df.iloc[index, 1]))
        if self.transform:
            image = self.transform(image)
    
            
        return (image, y_label)

"""Transforming the images in the required form."""

mean = [0.4, 0.5, 0.5]
std = [0.4, 0.5, 0.5]


transform = transforms.Compose(
    [transforms.Resize((100, 100)),
     #transforms.Grayscale(num_output_channels=3),
     transforms.ToTensor(),
    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))
    ])

dataset = ImageDataset(csv_file = image_cluster, root_dir = '/content/drive/MyDrive/SEMESTER6/DM/Images/', transform = transform)

"""The data was splitted in a roughly 70/30 manner. 70 being training size and 30 being testing size."""

train_set, test_set = torch.utils.data.random_split(dataset, [259,100])

"""Loading training and testing data"""

train_loader = DataLoader(dataset = train_set, batch_size = 16, shuffle=True)
test_loader = DataLoader(dataset = test_set, batch_size = 16, shuffle=True)

"""Neural network structure with 4 hidden layers and relu as activation function."""

class NN(nn.Module):    
    def __init__(self):
        super(NN, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(100*100*3, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 7)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

"""Entropy loss and Adam optimizer used."""

net = NN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

accuracies = []
time_taken = []
models = []

"""All of the neural networks are run till 20 epochs and time was stored along with accuracies and losses.

Training
"""

n_epochs=20
loss_list=[]
tot_loss=0

#n_epochs
start = time.process_time()
for epoch in range(n_epochs):
    print('epoch:',epoch)
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data 
        inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad() 
        z=net(inputs)
        loss=criterion(z,labels) 
        loss.backward()
        optimizer.step()
        loss_list.append(loss.data)
        tot_loss+=loss.data
        print('loss:', tot_loss/len(loss_list))

tt = time.process_time() - start
time_taken.append(tt/20)

"""Testing"""

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        images, labels = data
        images = images.to(device)
        labels = labels.to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy:', 100 * correct // total, '%')
accuracies.append(100 * correct // total)
models.append('Original ANN')

"""CNN"""

class NetC(nn.Module):
  def __init__(self):
    super(NetC, self).__init__()
    self.convo1 = nn.Conv2d(3, 10, 7)
    self.pool1 = nn.MaxPool2d(1, stride = 1)
    self.convo2 = nn.Conv2d(10, 21, 5)
    self.pool2 = nn.MaxPool2d(4, stride = 2)
    self.out = nn.Linear(40656, 7)


    

  def forward(self, x):
      # Pass the input tensor through each of our operations
      x = self.pool1(F.relu(self.convo1(x)))
      x = self.pool2(F.relu(self.convo2(x)))
      x = torch.flatten(x, 1)
      x = self.out(x)
      return x

cnet = NetC().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(cnet.parameters(), lr=0.001)

start = time.process_time()
n_epochs=20
loss_list=[]
tot_loss=0

#n_epochs
for epoch in range(n_epochs):
    print('epoch:', epoch)
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data 
        inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        z = cnet(inputs)
        loss=criterion(z,labels) 
        loss.backward()
        optimizer.step()
        loss_list.append(loss.data)
        tot_loss+=loss.data
        print('loss:', tot_loss/len(loss_list))
tt = time.process_time() - start
time_taken.append(tt/20)

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        images, labels = data
        images = images.to(device)
        labels = labels.to(device)
        outputs = cnet(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy:', 100 * correct // total, '%')
accuracies.append(100 * correct // total)
models.append('Original CNN')

"""Segmented Images"""

dataset2 = ImageDataset(csv_file = image_cluster, root_dir = '/content/drive/MyDrive/SEMESTER6/DM/PIL_SEG/', transform = transform)

"""ANN"""

train_set2, test_set2 = torch.utils.data.random_split(dataset2, [259,100])

train_loader2 = DataLoader(dataset = train_set2, batch_size = 16, shuffle=True)
test_loader2 = DataLoader(dataset = test_set2, batch_size = 16, shuffle=True)

class NN2(nn.Module):    
    def __init__(self):
        super(NN2, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(100*100*3, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 7)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

net2 = NN2().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net2.parameters(), lr=0.001)

n_epochs=20
loss_list=[]
tot_loss=0

#n_epochs
start = time.process_time()
for epoch in range(n_epochs):
    print('epoch:', epoch)
    for i, data in enumerate(train_loader2, 0):
        inputs, labels = data 
        inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad() 
        z=net2(inputs)
        loss=criterion(z,labels) 
        loss.backward()
        optimizer.step()
        loss_list.append(loss.data)
        tot_loss+=loss.data
        print('loss:', tot_loss/len(loss_list))
        #print(f'[{epoch+1}, {i+1:5d}] loss: {tot_loss/len(loss_list)}')

tt = time.process_time() - start
time_taken.append(tt/20)

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader2:
        images, labels = data
        images = images.to(device)
        labels = labels.to(device)
        outputs = net2(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy:', 100 * correct // total, '%')
accuracies.append(100 * correct // total)
models.append('Segmented ANN')

"""CNN"""

cnet2 = NetC().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(cnet2.parameters(), lr=0.001)

n_epochs=20
loss_list=[]
tot_loss=0

#n_epochs
start = time.process_time()
for epoch in range(n_epochs):
    for i, data in enumerate(train_loader2, 0):
        inputs, labels = data
        inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad() 
        z=cnet2(inputs)
        loss=criterion(z,labels) 
        loss.backward()
        optimizer.step()
        loss_list.append(loss.data)
        tot_loss+=loss.data
        #print('loss:', tot_loss/len(loss_list))
        print(f'[{epoch+1}, {i+1:5d}] loss: {tot_loss/len(loss_list)}')

tt = time.process_time() - start
time_taken.append(tt/20)

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader2:
        images, labels = data
        images = images.to(device)
        labels = labels.to(device)
        outputs = cnet2(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy:', 100 * correct // total, '%')
accuracies.append(100 * correct // total)
models.append('Segmented CNN')

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression

"""ML models

Flattening the images and resizing them so that we can use them as features in traditional ML models.
"""

ns = []
for i in image_cluster['image']:
  image = cv2.imread('/content/drive/MyDrive/SEMESTER6/DM/Images/' + i)
  image = cv2.resize(image, (64, 64), interpolation = cv2.INTER_AREA)
  # Change color to RGB (from BGR)
  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  ns.append(image.flatten())

s = []
for i in image_cluster['image']:
  image = cv2.imread('/content/drive/MyDrive/SEMESTER6/DM/PIL_SEG/' + i)
  image = cv2.resize(image, (64, 64), interpolation = cv2.INTER_AREA)
  # Change color to RGB (from BGR)
  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  s.append(image.flatten())

"""I used GuassianNB and Logistic Regression."""

def NB(x, y):
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    ada = GaussianNB()
    ada.fit(x_train, y_train)
    y_pred = ada.predict(x_test)
    return accuracy_score(y_pred, y_test)*100

def LOG(x, y):
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    ada = LogisticRegression()
    ada.fit(x_train, y_train)
    y_pred = ada.predict(x_test)
    return accuracy_score(y_pred, y_test)*100

start = time.process_time()
a = NB(ns, image_cluster['clusterid'])
tt = time.process_time() - start
time_taken.append(tt)
accuracies.append(a)
models.append('Original NB')

start = time.process_time()
a = NB(s, image_cluster['clusterid'])
tt = time.process_time() - start
time_taken.append(tt)
accuracies.append(a)
models.append('Segmented NB')

start = time.process_time()
a = LOG(ns, image_cluster['clusterid'])
tt = time.process_time() - start
time_taken.append(tt)
accuracies.append(a)
models.append('Original LR')

start = time.process_time()
a = LOG(s, image_cluster['clusterid'])
tt = time.process_time() - start
time_taken.append(tt)
accuracies.append(a)
models.append('Segmented LR')

"""Visualization"""

accuracies = accuracies[1:]

models = models[1:]

"""The following are the graphs that reflect each model's performance, including time taken, accuracies, and loss (for NN models)."""

from matplotlib.pyplot import figure

figure(figsize=(15, 6))
plt.bar(models,accuracies)
plt.xlabel('All Models')
plt.ylabel('Accuracies')
plt.title('Accuracies by each model')

figure(figsize=(15, 6))
plt.bar(models, time_taken)
plt.xlabel('All Models')
plt.ylabel('Time taken')
plt.title('Time taken by each model')

final_losses = [0.7016, 0.3490, 0.6634, 0.3680]
nn_models = ['Original ANN', 'Original CNN', 'Segmented ANN', 'Segmented CNN']
plt.bar(nn_models, final_losses)
plt.xlabel('NN models')
plt.ylabel('Final losses/Error report')
plt.title('Final losses of each NN model')

